# -*- coding: utf-8 -*-
"""zomato_rest_predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BmH8ajhzuBuHU3Ed6wKb9B8OFukNTScb
"""

# Code to read csv file into colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#2.1 Get the file
downloaded = drive.CreateFile({'id':'1b6HpkkpNjIyIOIfCzQIJ9kfZ76L-rA4i'}) # replace the id with id of file you want to access
downloaded.GetContentFile('zomato.csv')

"""# Predicting Zomato Restaurants Rate

#### Columns description

- **url** : contains the url of the restaurant in the zomato website

- **address** : contains the address of the restaurant in Bengaluru

- **name** : contains the name of the restaurant

- **online_order** : whether online ordering is available in the restaurant                      or not

- **book_table** : table book option available or not

- **rate** : contains the overall rating of the restaurant out of 5

- **votes** : contains total number of rating for the restaurant as of the                 above mentioned date

- **phone** : contains the phone number of the restaurant

- **location** : contains the neighborhood in which the restaurant is                        located

- **rest_type** : restaurant type

- **dish_liked** : dishes people liked in the restaurant

- **cuisines** : food styles, separated by comma

- **approx_cost(for two people)** : contains the approximate cost for meal                                       for two people

- **reviews_list** : list of tuples containing reviews for the restaurant,                        each tuple

- **menu_item** : contains list of menus available in the restaurant

- **listed_in(type)** : type of meal

- **listed_in(city)** : contains the neighborhood in which the restaurant is                         listed

## Importing Necessary Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import ast

from wordcloud import WordCloud, STOPWORDS
from sklearn.preprocessing import OneHotEncoder

from joblib import dump,load
#%matplotlib notebook
# %matplotlib inline


from sklearn.linear_model import LinearRegression
from sklearn import linear_model
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import  make_scorer
from sklearn.model_selection import GridSearchCV
from sklearn import metrics

import random
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestRegressor

import pickle


import warnings
warnings.filterwarnings('ignore')

"""## Understanding Data"""

# df = pd.read_csv('C:/Users/prash/Downloads/archive(3)/zomato.csv')
df = pd.read_csv('zomato.csv')

df.shape

df.head()

df.columns

df.info()

df.dtypes

df.describe()

"""#### Observations
- Minimum vote's value is 0, can be interpret as there are some restaurants who have 0 votes
- Maximum vote's value is 16832, there is a restaurant who has max   16832.
- Average vote's values is 284, so average 284 votes for restaurant

## Data Preprocessing

##### Dropping Unnecessary Columns.
##### And Adjusting column names for convininent

* Dropping address and listed_in(city) because they are representing same information.

* url, Phone columns not required so dropping these also.
"""

dropping_columns = ["address", "url", "listed_in(city)", "phone"]

df.drop(columns=dropping_columns, axis=1, inplace=True)

df.columns

"""##### Checking and Removing Duplicates"""

df.duplicated().sum()

print("No of Duplicates in dataset: ",df.duplicated().sum())

df.drop_duplicates(inplace=True)

# Removing Null alues

df.isnull().sum()

for col in df.columns:
    print("{} has  {} % missing values".format(col,round(df[col].isnull().sum()/len(df)*100,4)))

(df.isnull().sum()/len(df)*100).round(2)

df['rate'].unique()

df['rate'] = df['rate'].replace('NEW', np.NaN)
df['rate'] = df['rate'].replace('-', np.NaN)

df['rate'].unique()

(df.isnull().sum()/len(df)*100).round(2)

"""##### converting `rate` into `string` type and converting '4.1/5' to '4.1'  then again changing data type to `float`

"""

df['rate'] = df['rate'].astype(str)

df['rate'] = df['rate'].apply(lambda x: x.replace('/5', ''))

df['rate'] = df['rate'].apply(lambda x: float(x))

df['rate'].head()

type(df['reviews_list'][0])

df['reviews_list'] = df['reviews_list'].apply(lambda x: ast.literal_eval(x))
type(df['reviews_list'][0])

df['reviews_list'][0][0]

df['reviews_list'][0][1]

extracted = [float(i[0].replace('Rated','').strip()) for i in df['reviews_list'][0]]
extracted

extracted_mean = round((sum(extracted)/len(extracted)),1)
extracted_mean

print("Extracted Rate: ",extracted_mean)
print("Original Rate: ",df['rate'][0])

def extract_features_from_review_list(x):
    '''
    extract the rate value out of a string inside tuple
    '''
    # ensure that x is not Null and there is more than one rate
    if not x or len(x) <= 1:
        return None
    
    ## checked give values is text or not, if it is text then removed 'Rated', convert string to float.
    rate = [float(i[0].replace('Rated','').strip())  for i in x if type(i[0])== str]
    
    ## return average value
    return round((sum(rate)/len(rate)),1)

# create new column
df['review_rate']  = df['reviews_list'].apply(lambda x : extract_features_from_review_list(x))

## Compare "Original Rate" vs "Rate extracted from Review List"
df.loc[:,['rate','review_rate']].sample(10,random_state=1)

df.rate.isna().sum()

## finding rate = NaN and review_rate == not NaN
df.query('rate != rate & review_rate == review_rate')[:5]

## finding index where rate==NaN and review_rate== not NaN
nan_index = df.query('rate != rate & review_rate == review_rate').index
for i in nan_index:
    df.loc[i,'rate'] = df.loc[i,'review_rate']

df.rate.isna().sum()

((df.isna().sum()/len(df))*100).round(2)

df.drop(columns='review_rate',axis=1,inplace=True)

df.rate.isna().sum()

# drop null values
df.dropna(subset=['rate', 'approx_cost(for two people)'],inplace=True)

df.shape

df.isnull().sum()

df = df[df['cuisines'].isnull()==False]

df.rename(columns={'approx_cost(for two people)': 'average_cost'}, inplace=True)

((df.isnull().sum()/len(df))*100).round(2)

df['dish_liked'] = df['dish_liked'].apply(lambda x:x.lower().strip() if isinstance(x,str) else x)

df['dish_liked'][:2]

menu_list = []

# collect the dishes' names and make a menu list for all kind of dishes
for dish in df.dish_liked.tolist():
    if isinstance(dish,str) and len(dish)>0:
        for e in dish.split(','):
            menu_list.append(e)
len(menu_list)

# Now collect the unique dish name 
menu_set = set(menu_list)

def clear_text(t):
    return ' '.join([i[1] for i in t]).encode('utf8').decode('ascii',errors='replace')

df['process_text'] = df['reviews_list'].apply(lambda x: clear_text(x))

df['process_text']

# here we removed all above mentioned characters

df['process_text'] = df['process_text'].apply(lambda x : x.replace("RATED\n  ",'').replace('?','').replace('ï¿½','').replace('\n','').replace('.',' ').strip().lower())

df['process_text'][0]

df['dish_liked'].nunique()

# make lower case
df['dish_liked'] = df['dish_liked'].apply(lambda x:x.lower().strip() if isinstance(x,str) else x)

df['dish_liked'][10000]

menu_set.intersection(df['process_text'][10000].split(' '))

df['dish_n_review'] = df['process_text'].apply(lambda x: ', '.join(list(menu_set.intersection(x.split(' ')))))

df.query('dish_liked != dish_liked')[['dish_liked','dish_n_review']].sample(5,random_state=1)

nan_index = df.query('dish_liked != dish_liked & dish_n_review == dish_n_review').index
for i in nan_index:
    df.loc[i,'dish_liked'] = df.loc[i,'dish_n_review']

df['dish_liked'][10000]

del menu_list
del menu_set

df.drop(columns=['dish_n_review'],axis=1,inplace=True)

((df.isnull().sum()/len(df))*100).round(4)

df.shape

"""### Visualizations"""

df['rate'].hist(color = 'blue')
plt.axvline(x=df['rate'].mean(), ls = '--', color = 'red')
plt.title("Average Rating for Bangalore Restaurants",weight = 'bold')
plt.xlabel("Ratings")
plt.ylabel("No. of Restaurants")
print("Mean is :",df['rate'].mean());

df['name'].value_counts().head()

plt.figure(figsize=(12,6))
ax = df['name'].value_counts()[:20].plot(kind='bar', colormap= 'gist_rainbow')
ax.legend(['* Restaurants'])
plt.xlabel('No of Restaurants')
plt.ylabel('Count of Restaurant')
plt.title("Name Vs Number of Restaurants", fontsize=20, weight='bold');

plt.figure(figsize=(8,3))
ax =df['online_order'].value_counts().plot(kind='bar', color={'blue','orange'})
plt.title('Number of Restaurants accepting online orders', weight='bold')
plt.xlabel('online orders')
plt.ylabel('counts')

df['online_order'].value_counts()

plt.figure(figsize=(8,3))
ax =df['book_table'].value_counts().plot(kind='bar', color={'blue','orange'})
plt.title('Number of Restaurants has book table option', weight='bold')
plt.xlabel('book table facility')
plt.ylabel('counts')

df['book_table'].value_counts()

plt.figure(figsize=(8,8))
ax =df['location'].value_counts().plot(kind='pie')
plt.title('Location', weight='bold');

plt.figure(figsize=(8,8))
ax =df['location'].value_counts()[:10].plot(kind='pie')
plt.title('Location', weight='bold')

## https://stackoverflow.com/questions/6170246/how-do-i-use-matplotlib-autopct

plt.figure(figsize=(8,8))
values = df['location'].value_counts()[:10]
labels = df['location'].value_counts()[:10].index
plt.pie(values, labels=labels, autopct='%.2f')
plt.title('Location percentage', weight='bold')
plt.show()

plt.figure(figsize=(8,3))
ax =df['location'].value_counts()[:10].plot(kind='bar')
plt.title('Number of Restaurants in given location', weight='bold')
plt.xlabel('Area')
plt.ylabel('counts')

df['location'].nunique()

plt.figure(figsize=(8,8))
values = df['rest_type'].value_counts()[:10]
labels = df['rest_type'].value_counts()[:10].index
plt.pie(values, labels=labels, autopct='%.2f')
plt.title('Type of Restaurant in City(%) ', weight='bold')
plt.show()

plt.figure(figsize=(10,3))
ax =df['rest_type'].value_counts()[:10].plot(kind='bar')
plt.title('Number of Restaurants in given location', weight='bold')
plt.xlabel('Area')
plt.ylabel('counts')

plt.figure(figsize=(8,8))
values = df['average_cost'].value_counts()[:10]
labels = df['average_cost'].value_counts()[:10].index
plt.pie(values, labels=labels, autopct='%.2f')
plt.title('Average cost for two person(in %) ', weight='bold')
plt.show()

df['dish_liked'].nunique()

#lets delete the nulll values

data1 = df.copy()

dishes_data = data1[data1.dish_liked.notnull()]
dishes_data.dish_liked = dishes_data.dish_liked.apply(lambda x:x.lower().strip())

dishes_data.isnull().sum()

dishes_data.dish_liked[:10]

# count each dish to see how many times each dish repeated
dish_count = []
for i in dishes_data.dish_liked: ## iterate in each rows in table
    for t in i.split(','):
        t = t.strip() # remove the white spaces to get accurate results
        dish_count.append(t)

dish_count[:10]

plt.figure(figsize=(12,6)) 
pd.Series(dish_count).value_counts()[:10].plot(kind='bar')
plt.title('Top 10 dished_liked in Bangalore',weight='bold')
plt.xlabel('Dish')
plt.ylabel('Count')

dish_set = set(dish_count)
dish_word_cloud = ', '.join(dish_set)

plt.figure( figsize=(15,10) )
wc = WordCloud(width=1600, height=1000,background_color="black", max_words=len(dish_word_cloud))
wc.generate(dish_word_cloud)
plt.imshow(wc, interpolation='bilinear')
plt.title('Word Cloud for favourite dishes',weight='bold')
plt.axis("off")
plt.imshow(wc)
plt.show()

f,ax=plt.subplots(figsize=(18,8))
g = sns.pointplot(x=df["rest_type"], y=df["rate"], data=df)
g.set_xticklabels(g.get_xticklabels(), rotation=90)
plt.title('Restaurent type vs Rate', weight = 'bold')
plt.show()

cuisines_data = df[df.cuisines.notnull()]
cuisines_data.cuisines = cuisines_data.cuisines.apply(lambda x:x.lower().strip())

cuisines_count= []

for i in cuisines_data.cuisines:
    for j in i.split(','):
        j = j.strip()
        cuisines_count.append(j)

plt.figure(figsize=(12,6)) 
pd.Series(cuisines_count).value_counts()[:10].plot(kind='bar',color= 'r')
plt.title('Top 10 cuisines in Bangalore',weight='bold')
plt.xlabel('cuisines type')
plt.ylabel('No of restaurants');

cuisines_set = set(cuisines_count)
cuisines_word_cloud = ', '.join(cuisines_set)

plt.figure( figsize=(15,10) )
wc = WordCloud(width=1600, height=1000,background_color="black", max_words=len(cuisines_word_cloud))
wc.generate(cuisines_word_cloud)
plt.title('Word Cloud for cuisines',weight='bold')
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.imshow(wc)
plt.show()

plt.figure(figsize = (12,6))
sns.countplot(x=df['rate'], hue = df['online_order'])
plt.ylabel("Restaurants that Accept/Not Accepting online orders")
plt.title("rate vs oline order",weight = 'bold');

df['online_order']= pd.get_dummies(df['online_order'], drop_first=True)
df['book_table']= pd.get_dummies(df['book_table'], drop_first=True)
df

df.columns

# drop redudant columns
df.drop(columns=['dish_liked','reviews_list','menu_item','listed_in(type)'], inplace  =True)

# removed ',' between two words; then converted to categorical features
df['rest_type'] = df['rest_type'].str.replace(',' , '') 
df['rest_type'] = df['rest_type'].astype(str).apply(lambda x: ' '.join(sorted(x.split())))
df['rest_type'].value_counts().head()

# removed ',' between two words; then converted to categorical features

df['cuisines'] = df['cuisines'].str.replace(',' , '') 
df['cuisines'] = df['cuisines'].astype(str).apply(lambda x: ' '.join(sorted(x.split())))
df['cuisines'].value_counts().head()

# removed ',' between two words; then converted to string to int

df['average_cost'] = df['average_cost'].str.replace(',' , '') 
df['average_cost'] = df['average_cost'].apply(int)

df.head(3)

# build x & y dataset
x = df.drop(['rate','name','process_text'],axis = 1)
y = df['rate']

x.head()

y.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 33)

# save to local disk
print(dump(X_train, filename='pkl_files_X_train'))
print(dump(X_test, filename='pkl_files_X_test'))
print(dump(y_train, filename='pkl_files_y_train'))
print(dump(y_test, filename='pkl_files_y_test'))

X_train = load('pkl_files_X_train')
X_test = load('pkl_files_X_test')
y_train = load('pkl_files_y_train')
y_test = load('pkl_files_y_test')

# X_train.head(2)

enc = OneHotEncoder( handle_unknown='ignore')

## ALWAYS AVOID DATA LEAKAGE

# this method is for training data set
def one_hot_fit_transform(df,name):
    output_data = df[name].values.reshape(-1, 1)
    return enc.fit_transform(output_data).toarray()

# this method is for test data set
def one_hot_transform(df,name):
    output_data1 = df[name].values.reshape(-1, 1)
    return enc.transform(output_data1).toarray()

# one hot encoding apply to 'rest_type' features on train/test dataset
tr_dummy_rest_type = one_hot_fit_transform(X_train,'rest_type' )
te_dummy_rest_type= one_hot_transform(X_test,'rest_type' )

# one hot encoding apply to 'location' features on train/test dataset
tr_dummy_city = one_hot_fit_transform(X_train,'location' )
te_dummy_city= one_hot_transform(X_test,'location')

# one hot encoding apply to 'cuisines' features on train/test dataset
tr_dummy_cuisines = one_hot_fit_transform(X_train,'cuisines' )
te_dummy_cuisines=one_hot_transform(X_test,'cuisines')

tr_dummy_rest_type.shape, te_dummy_rest_type.shape

tr_dummy_city.shape, te_dummy_city.shape

tr_dummy_cuisines.shape, te_dummy_cuisines.shape

## combine all 'one-hot' encoded features as Tr.
tr =pd.DataFrame(pd.np.column_stack([ tr_dummy_rest_type,tr_dummy_city, tr_dummy_cuisines]))

## CONCAT both dataframe ### ie Tr and X_train(original dataframe)
## https://stackoverflow.com/questions/45963799/pandas-concat-resulting-in-nan-rows

l1=X_train.values.tolist()
l2=tr.values.tolist()

for i in range(len(l1)):
    l1[i].extend(l2[i])

X_train=pd.DataFrame(l1,columns=X_train.columns.tolist()+tr.columns.tolist())
X_train.shape

## combine all 'one-hot' encoded features as Te.
te =pd.DataFrame(pd.np.column_stack([ te_dummy_rest_type,te_dummy_city,te_dummy_cuisines]))

## CONCAT both dataframe ### ie Te and X_test(original dataframe)
## https://stackoverflow.com/questions/45963799/pandas-concat-resulting-in-nan-rows

l3=X_test.values.tolist()
l4=te.values.tolist()
for i in range(len(l3)):
    l3[i].extend(l4[i])

X_test=pd.DataFrame(l3,columns=X_test.columns.tolist()+te.columns.tolist())
X_test.shape

# after onehot encoding DONE. 'location','rest_type','cuisines' are redundant features. REMOVE them.

X_train =X_train.drop(['location','rest_type','cuisines'],axis = 1)
X_test =X_test.drop(['location','rest_type','cuisines'],axis = 1)

X_train.head(3)

print(x.shape)
print(y.shape)

# checking final train set shape
X_train.shape, y_train.shape

# # checking final test set shape
X_test.shape, y_test.shape

print(dump(X_train, 'max_features_pkl_X_train'))
print(dump(X_test, 'max_features_pkl_X_test'))
print(dump(y_train, 'max_features_pkl_y_train'))
print(dump(y_test, 'max_features_pkl_y_test'))

X_train= load('max_features_pkl_X_train') 
X_test= load('max_features_pkl_X_test')
y_train= load('max_features_pkl_y_train')
y_test= load('max_features_pkl_y_test')

from sklearn.metrics import  make_scorer
from sklearn.model_selection import GridSearchCV
from sklearn import metrics


def mse(y, y_pred):
    return np.mean((y_pred - y)**2) 

mse_scorer = make_scorer(mse, greater_is_better=False)

import random

rand_pred= np.zeros(y_test.shape[0])
for i in range(y_test.shape[0]):
    rand_probs = round(random.uniform(1.0, 5.0),2)
    rand_pred[i] = rand_probs

mse(y_test, rand_pred)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train,y_train)
y_pred_lr = lr.predict(X_test)

mse(y_test, y_pred_lr)

from sklearn import linear_model

sgdReg = linear_model.SGDRegressor()
sgdReg.fit(X_train,y_train)
y_pred_sgdr = sgdReg.predict(X_test)

mse(y_test, y_pred_sgdr)

from xgboost import XGBClassifier

xgb = XGBClassifier(n_jobs=-1, eval_metric='rmse', random_state=1,verbose_eval=10)
xgb.fit(X_train,y_train)
y_pred_xgb = xgb.predict(X_test)

mse(y_test, y_pred_xgb)

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor()
rfr.fit(X_train,y_train)
y_pred_rfr = rfr.predict(X_test)

mse(y_test, y_pred_rfr)

# tuned_parameters = {'n_estimators': [250,500,1000,1200]}

# grd_regressor = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=10, 
#                    n_jobs=-1, verbose=1, scoring=mse_scorer)
# grd_regressor.fit(X_train, y_train)

rfr = RandomForestRegressor(max_depth=None,n_estimators=200,min_samples_split= 2)
rfr.fit(X_train,y_train)
y_pred_rfr = rfr.predict(X_test)

mse(y_test, y_pred_rfr)

Rand_pred = pd.DataFrame({ "actual": y_test, "pred": y_pred_rfr })
Rand_pred

# Use pickle to save our model so that we can use it later

import pickle 
# Saving model to disk
pickle.dump(rfr, open('model.pkl','wb'))
model=pickle.load(open('model.pkl','rb'))

